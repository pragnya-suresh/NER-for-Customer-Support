{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path=\"/Users/pragnyasuresh/Desktop/team_no_8/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy\n",
    "from textacy import lexicon_methods\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import csv\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords \n",
    "import math\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from numpy import array \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "ps = PorterStemmer()\n",
    "from spacy.parts_of_speech import ADJ, ADV, NOUN, VERB\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Flatten\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import MaxPooling1D\n",
    "from os import path\n",
    "from __future__ import unicode_literals, print_function\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import scipy.sparse as sps\n",
    "from spacy.lang.en import English\n",
    "from ahocorasick import Automaton\n",
    "from spacy.util import minibatch, compounding\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''train a model  to categorize the reviews based on positive and negative sentiments. So output of svm will be 1  and 1\n",
    "0-negative\n",
    "Take the negative sentiment reviews and apply ner\n",
    "These are the reviews that have issues\n",
    "Once the entities have been recognised, the review can be routed to the corresponding dept\n",
    "\n",
    "Ex:  Review: few weeks ago I ordered some cotton tops. They sent me some cheap nylon tops instead. \n",
    "     Entities : (PRODUCT,top), (INFO,cotton),(INFO,nylon)\n",
    "     Response :What was the brand of the top?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Categories:\n",
    "#1. Clothing\n",
    "#2. Phones\n",
    "#3. Home\n",
    "#4. Footwear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path=\"/Users/pragnyasuresh/Desktop/team_no_8/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "#Extracting 4000 reviews from each category and performing preprocessing-stop word removal and lemmatization \n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "with open(path + \"Clothing_Accessories.txt\", 'r') as f:\n",
    "    with open(path + \"reviews.csv\", 'w') as writeFile:\n",
    "        original_reviews=[]\n",
    "        writer = csv.writer(writeFile)\n",
    "        header=[\"department\",\"sentiment\",\"review\",\"original_review\"]\n",
    "        writer.writerow(header)\n",
    "        ctr=0\n",
    "        for line in f:\n",
    "            if(line.find(\"review/score:\")!=-1):\n",
    "                rev=[\"Clothing_Accessories\",line[13:]]\n",
    "            if(line.find(\"review/text:\")!=-1 and ctr<4000):\n",
    "                review=str(line[13:]).lower()\n",
    "                #print(review)\n",
    "                lll = [lemmatizer.lemmatize(w).lower() for w in review.split(' ') if w.lower() not in stop_words]\n",
    "                rev.append(' '.join(lll))\n",
    "                original_reviews.append(review)\n",
    "                #print(rev)\n",
    "                rev.append(review)\n",
    "                writer = csv.writer(writeFile)\n",
    "                writer.writerow(rev)\n",
    "                ctr+=1\n",
    "    print(ctr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "with open(path+\"Cell_Phones_Accessories.txt\", 'r') as f:\n",
    "    with open(path+\"reviews.csv\", 'a') as writeFile:\n",
    "        ctr=0\n",
    "        for line in f:\n",
    "            if(line.find(\"review/score:\")!=-1):\n",
    "                rev=[\"Cell_Phones_Accessories\",line[13:]]\n",
    "            if(line.find(\"review/text:\")!=-1 and ctr<4000):    \n",
    "                review=str(line[13:]).lower()\n",
    "                #print(review)\n",
    "                lll = [lemmatizer.lemmatize(w).lower() for w in review.split(' ') if w.lower() not in stop_words]\n",
    "                rev.append(' '.join(lll))\n",
    "                original_reviews.append(review)\n",
    "                rev.append(review)\n",
    "                writer = csv.writer(writeFile)\n",
    "                writer.writerow(rev)\n",
    "                ctr+=1\n",
    "            \n",
    "            \n",
    "    print(ctr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "with open(path+\"Home_Kitchen.txt\", 'r') as f:\n",
    "    with open(path+\"reviews.csv\", 'a') as writeFile:\n",
    "        ctr=0\n",
    "        for line in f:\n",
    "            if(line.find(\"review/score:\")!=-1):\n",
    "                    rev=[\"Home_Kitchen\",line[13:]]\n",
    "            if(line.find(\"review/text:\")!=-1 and ctr<4000):\n",
    "                review=str(line[13:]).lower()\n",
    "                lll = [lemmatizer.lemmatize(w).lower() for w in review.split(' ') if w.lower() not in stop_words]\n",
    "                rev.append(' '.join(lll))\n",
    "                original_reviews.append(review)\n",
    "                rev.append(review)\n",
    "                writer = csv.writer(writeFile)\n",
    "                \n",
    "                writer.writerow(rev)\n",
    "                ctr+=1\n",
    "            \n",
    "            \n",
    "    print(ctr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n",
      "5.0    6064\n",
      "4.0    2349\n",
      "1.0    1530\n",
      "3.0    1120\n",
      "2.0     937\n",
      "Name: sentiment, dtype: int64\n",
      "10880\n",
      "1.0    8413\n",
      "0.0    2467\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Drop all the reviews with rating 3. Update reviews with rating 1 & 2 as 0 rating. Update Reviews with\n",
    "#4 and 5 as 1 rating. We are doing this to make it a binary classification.\n",
    "df=pd.read_csv(path+\"reviews.csv\", delimiter = ',')\n",
    "print(len(df))\n",
    "print(df['sentiment'].value_counts())\n",
    "\n",
    "#Drop all the reviews with rating 3\n",
    "df = df[df.sentiment != 3]\n",
    "print(len(df))\n",
    "df['sentiment']=df['sentiment'].replace([1, 2], 0)\n",
    "df['sentiment']=df['sentiment'].replace([4, 5], 1)\n",
    "print(df['sentiment'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3773 2467\n",
      "Clothing_Accessories       3650\n",
      "Cell_Phones_Accessories    2021\n",
      "Home_Kitchen                569\n",
      "Name: department, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Making the dataset balanced by dropping reviews of the class that is more in number.\n",
    "df1=df.loc[df['sentiment'] == 1]\n",
    "df2=df.loc[df['sentiment'] == 0]\n",
    "\n",
    "df1=df1[:3773]\n",
    "print(len(df1),len(df2))\n",
    "\n",
    "df=df1.append(df2, ignore_index=True)\n",
    "print(df['department'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating sparse vector using TFID vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "sparse_vector = vectorizer.fit_transform(df[\"review\"])\n",
    "n_features=sparse_vector.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using sparse vector representation of each review = 92.2275641025641\n"
     ]
    }
   ],
   "source": [
    "#Using SVM to classify reviews as positive or negative.\n",
    "#Since we needed to manually annotate the corpus for training the ner, we could not use a huge corpus\n",
    "#ANN needs huge amount of data to provide good accuracy.\n",
    "#SVM classification gave much better accuracy whereas ANN gave about 72% so we decided to use SVM to classify the reviews\n",
    "X_train, X_test, y_train, y_test = train_test_split(sparse_vector, df[\"sentiment\"], test_size=0.4, random_state=42)\n",
    "l1=list(df[\"department\"])\n",
    "l2=list(df[\"review\"])\n",
    "l3=list(df[\"original_review\"])\n",
    "l=[]\n",
    "for i in range(len(l1)):\n",
    "    l.append([l1[i],l2[i],l3[i]])\n",
    "\n",
    "#need to map the reviews and department to the review. To do this, the line below is used. It will split the same way\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(l, df[\"sentiment\"], test_size=0.4, random_state=42)\n",
    "clf = SVC(kernel='linear') \n",
    "# fitting x samples and y classes \n",
    "clf.fit(X_train, y_train) \n",
    "#Predicting and evaluatng accuracy\n",
    "\n",
    "y_pred_list=clf.predict(X_test)\n",
    "ans=array(y_pred_list)\n",
    "acc = sklearn.metrics.accuracy_score(y_test, y_pred_list)\n",
    "print(\"Accuracy using sparse vector representation of each review =\",acc*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    1567\n",
      "0.0     929\n",
      "Name: sent, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Cell_Phones_Accessories    458\n",
       "Clothing_Accessories       246\n",
       "Home_Kitchen               225\n",
       "Name: department, dtype: int64"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we now extract all reviews with a negative sentiment to perform NER on them\n",
    "y_pred_list=list(y_pred_list)\n",
    "df_pred = pd.DataFrame({'sent':y_pred_list})\n",
    "\n",
    "print (df_pred[\"sent\"].value_counts())\n",
    "X_test2=list(X_test2)\n",
    "negative_reviews=[]\n",
    "j=0\n",
    "with open(path+\"negative_reviews.csv\", 'w') as writeFile:\n",
    "    header=[\"department\",\"review\",\"original_review\"]\n",
    "    writer = csv.writer(writeFile)\n",
    "    writer.writerow(header)\n",
    "    for i in y_pred_list:\n",
    "    \n",
    "        if(i==0.0):\n",
    "            #print(i,j)\n",
    "            negative_reviews=[X_test2[j][0]]\n",
    "            negative_reviews.append(X_test2[j][1])\n",
    "            negative_reviews.append(X_test2[j][2])\n",
    "            #print(negative_reviews)\n",
    "            writer = csv.writer(writeFile)\n",
    "            writer.writerow(negative_reviews)\n",
    "        j+=1\n",
    "df_neg = pd.read_csv(path+\"negative_reviews.csv\") \n",
    "df_neg[\"department\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the train and test datasets for the NER model (750-train, 258-test)\n",
    "df_neg['department']=df_neg['department'].replace(\"Clothing_Accessories\", 1)\n",
    "df_neg['department']=df_neg['department'].replace(\"Cell_Phones_Accessories\", 2)\n",
    "df_neg['department']=df_neg['department'].replace(\"Home_Kitchen\", 3)\n",
    "train= df_neg.iloc[:750, :]\n",
    "test = df_neg.iloc[750:,:]\n",
    "test=df_neg.drop([\"department\"],axis=1)\n",
    "train.to_csv(path+\"train_ner.csv\",index=False)\n",
    "test.to_csv(path+\"test_ner.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform ner on negative_reviews - training the NER model\n",
    "from __future__ import unicode_literals, print_function\n",
    "import patterns\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "df_ent=pd.read_csv(path+\"train_ner.csv\")\n",
    "e=df_ent[\"review\"]\n",
    "e=e.values.tolist()\n",
    "TRAIN_DATA = []\n",
    "patterns.A.make_automaton()\n",
    "\n",
    "for x in e:\n",
    "    list_ent=[]\n",
    "    d={}\n",
    "    for item in patterns.A.iter(x):\n",
    "        val=((item[0]-len(item[1][1]))+1,item[0]+1,item[1][0])\n",
    "        list_ent.append(val)\n",
    "    d[\"entities\"]=list_ent\n",
    "    TRAIN_DATA.append((x,d))\n",
    "\n",
    "\n",
    "df_test=pd.read_csv(path+\"test_ner.csv\")\n",
    "test=df_test[\"review\"]\n",
    "TEST_DATA=list(test)\n",
    "\n",
    "\n",
    "@plac.annotations(\n",
    "                  model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "                  output_dir=(\"/Users/pragnyasuresh/Desktop/team_no_8\", \"option\", \"o\", Path),\n",
    "                  n_iter=(\"Number of training iterations\", \"option\", \"n\", int),\n",
    "                  )\n",
    "def main(model=None, output_dir=path, n_iter=100):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "    \n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "    \n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            #print(ent)\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        # reset and initialize the weights randomly – but only if we're\n",
    "        # training a new model\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "            for itn in range(n_iter):\n",
    "                random.shuffle(TRAIN_DATA)\n",
    "                losses = {}\n",
    "                # batch up the examples using spaCy's minibatch\n",
    "                batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "                for batch in batches:\n",
    "                    texts, annotations = zip(*batch)\n",
    "                    nlp.update(\n",
    "                               texts,  # batch of texts\n",
    "                               annotations,  # batch of annotations\n",
    "                               drop=0.5,  # dropout - make it harder to memorise data\n",
    "                               losses=losses,\n",
    "                               )\n",
    "                Lprint(\"Losses\", losses)\n",
    "\n",
    "\n",
    "    # test the trained model\n",
    "    l=[]\n",
    "    l2=[]\n",
    "    for text  in TEST_DATA:\n",
    "        doc = nlp(text)\n",
    "        #print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "        for ent in doc.ents:\n",
    "            l.append(ent.text)\n",
    "            l2.append(ent.label_)\n",
    "\n",
    "        df = pd.DataFrame({'ent':l,'label':l2})\n",
    "        df.to_csv(path+\"test.csv\",index=False)\n",
    "#print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "        \n",
    "        # save model to output directoryf\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "        \n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        for text, _ in TRAIN_DATA:\n",
    "            doc = nlp2(text)\n",
    "            print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "#print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    plac.call(main)\n",
    "\n",
    "'''\n",
    "    it’s not enough to only show a model a single example once. Especially if you only have few examples, you’ll want to train for a number of iterations. At each iteration, the training data is shuffled to ensure the model doesn’t make any generalizations based on the order of examples. Another technique to improve the learning results is to set a dropout rate, a rate at which to randomly “drop” individual features and representations. This makes it harder for the model to memorize the training data. For example, a 0.25 dropout means that each feature or internal representation has a 1/4 likelihood of being dropped.'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the NER model after training\n",
    "from __future__ import unicode_literals, print_function\n",
    "import patterns\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import pandas as pd\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "\n",
    "df_test=pd.read_csv(path+\"test_ner.csv\")\n",
    "test=df_test[\"review\"]\n",
    "TEST_DATA=list(test)\n",
    "\n",
    "@plac.annotations(\n",
    "                  model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
    "                  output_dir=(\"/Users/pragnyasuresh/Desktop/team_no_8\", \"option\", \"o\", Path),\n",
    "                  n_iter=(\"Number of training iterations\", \"option\", \"n\", int),\n",
    "                  )\n",
    "def main(model=None, output_dir=\"/Users/pragnyasuresh/Desktop/team_no_8\", n_iter=100):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    print(\"Loading from\", output_dir)\n",
    "    nlp2 = spacy.load(output_dir)\n",
    "    # test the trained model\n",
    "    l=[]\n",
    "    l2=[]\n",
    "\n",
    "    with open('/Users/pragnyasuresh/Desktop/team_no_8/test.csv', 'w') as writeFile:\n",
    "        writer = csv.writer(writeFile)\n",
    "        header=[\"Entity\",\"Label\",\"Review\"]\n",
    "        writer.writerow(header)\n",
    "        for text  in TEST_DATA:\n",
    "            doc = nlp2(text)\n",
    "            l3=text\n",
    "            #print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "            l1=[]\n",
    "            l2=[]\n",
    "            for ent in doc.ents:\n",
    "                if(ent.text not in l1):\n",
    "                    l1.append(ent.text)\n",
    "                    l2.append(ent.label_)\n",
    "        \n",
    "            l=[','.join(l1),','.join(l2),l3]\n",
    "            print(l)\n",
    "            writer.writerow(l)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    plac.call(main)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Departments = [\"Clothing\",\"Footwear\",\"Home\",\"Phones\"]\n",
    "Clothing = [\"shirt\",\"tee\",\"jeans\",\"pant\",\"tshirt\",\"t-shirt\",\"top\",\"shorts\",\"saree\",\"fabric\",\"skirt\",\"joggers\",\"dress\"]\n",
    "Footwear = [\"shoe\",\"slipper\",\"heels\",\"bellies\",\"sandals\",\"boots\",\"shoes\"]\n",
    "Home = [\"pan\",\"cooker\",\"warmer\",\"coffee\",\"mug\",\"cup\",\"china\",\"lunchbox\",\"fan\",\"mixer\",\"juicer\",\"curtain\",\"cookware\"]\n",
    "Phones = [\"phone\",\"charger\",\"adapter\",\"plug\",\"cable\",\"cord\",\"earset\",\"ear\",\"buds\",\"display\",\"case\",\"battery\",\"headset\",\"earphones\",\"bluetooth\",\"cover\"]\n",
    "Ent=Clothing+Footwear+Home+Phones\n",
    "Department = [Clothing,Footwear,Home,Phones]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    department                                            meaning\n",
      "0            0                 shirt garment worn upper half body\n",
      "1            0  tee support holding football end ground prepar...\n",
      "2            0  jeans (usually plural) close-fitting trousers ...\n",
      "3            0  pant (usually plural) garment extending waist ...\n",
      "4            0                                             tshirt\n",
      "5            0               t-shirt close-fitting pullover shirt\n",
      "6            0  top garment (especially women) extends shoulde...\n",
      "7            0                           shorts trousers end knee\n",
      "8            0  saree dress worn primarily hindu women; consis...\n",
      "9            0  fabric artifact made weaving felting knitting ...\n",
      "10           0  skirt garment hanging waist; worn mainly girls...\n",
      "11           0  joggers someone runs steady slow pace (usually...\n",
      "12           0  dress clothing distinctive style particular oc...\n",
      "0            1  shoe footwear shaped fit foot (below ankle) fl...\n",
      "1            1  slipper low footwear slipped easily; usually w...\n",
      "2            1             heels (golf) part clubhead joins shaft\n",
      "3            1                    bellies hollow inside something\n",
      "4            1  sandals shoe consisting sole fastened straps foot\n",
      "5            1         boots footwear covers whole foot lower leg\n",
      "6            1  shoes footwear shaped fit foot (below ankle) f...\n",
      "0            2                   pan shallow container made metal\n",
      "1            2                             cooker utensil cooking\n",
      "2            2     warmer device heats water supplies warmth room\n",
      "3            2  coffee several small trees shrubs native tropi...\n",
      "4            2            mug person gullible easy take advantage\n",
      "5            2  cup small open container usually used drinking...\n",
      "6            2  china communist nation covers vast territory e...\n",
      "7            2                                           lunchbox\n",
      "8            2  fan device creating current air movement surfa...\n",
      "9            2  mixer electronic equipment mixes two input sig...\n",
      "10           2  juicer person drinks alcoholic beverages (espe...\n",
      "11           2  curtain hanging cloth used blind (especially w...\n",
      "12           2  cookware kitchen utensil made material melt ea...\n",
      "0            3  phone electronic equipment converts sound elec...\n",
      "1            3       charger device charging recharging batteries\n",
      "2            3  adapter device enables something used way diff...\n",
      "3            3  plug electrical device fits cylinder head inte...\n",
      "4            3  cable conductor transmitting electrical optica...\n",
      "5            3       cord light insulated conductor household use\n",
      "6            3                                             earset\n",
      "7            3    ear fruiting spike cereal plant especially corn\n",
      "8            3  buds swelling plant stem consisting overlappin...\n",
      "9            3  display electronic device represents informati...\n",
      "10           3   case portable container carrying several objects\n",
      "11           3  battery device produces electricity; may sever...\n",
      "12           3        headset receiver consisting pair headphones\n",
      "13           3  earphones electro-acoustic transducer converti...\n",
      "14           3                                          bluetooth\n",
      "15           3  cover recording song first recorded made popul...\n"
     ]
    }
   ],
   "source": [
    "#Obtaining synset meanings for the words that describe each department and providing them as input to train Naive Bayes\n",
    "#Classifier\n",
    "stop_words = set(stopwords.words('english'))\n",
    "dfdept = []\n",
    "for department in range(len(Departments)):\n",
    "    w1=wn.synsets(Departments[department])[0]\n",
    "    dept = []\n",
    "    for ele in Department[department]:\n",
    "        w2=wn.synsets(ele)\n",
    "        #w1.wup_similarity()\n",
    "        if(len(w2)>0):\n",
    "            maxsim = 0\n",
    "            maxindex = -1\n",
    "            for i in range(len(w2)):\n",
    "                #print(w1,w2[i],i)\n",
    "                if(w2[i].name().split('.')[1] == 'n'):\n",
    "                    sim = w1.wup_similarity(w2[i])\n",
    "                    #print(sim)\n",
    "                    if(type(sim) is float and sim > maxsim):\n",
    "                        maxsim = sim\n",
    "                        maxindex = i\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            string = ele + \" \" + w2[maxindex].definition()\n",
    "            #print(\"string:\",string.split(' '))\n",
    "            lll = [w.lower() for w in string.split(' ') if w.lower() not in stop_words]\n",
    "            dept.append(' '.join(lll))\n",
    "            #dept.append(string)\n",
    "            #print(dept)\n",
    "            \n",
    "        else:\n",
    "            dept.append(ele)\n",
    "            \n",
    "    dfdept.append(dept)\n",
    "#print(\"dfdept:\",dfdept[0])\n",
    "#df.{\"meaning\":\"a garment\" , \"department\":0k}\n",
    "\n",
    "df_train = pd.DataFrame({'meaning':dfdept[0], 'department': 0})\n",
    "#df_train = df.append(pd.DataFrame({'meaning':dfdept[1], 'department': 1}))\n",
    "\n",
    "for i in range(1,len(dfdept)):\n",
    "    df2 = pd.DataFrame({'meaning':dfdept[i], 'department': i})\n",
    "    df_train = df_train.append(df2)\n",
    "#df_train=df_train.sample(frac=1)\n",
    "print(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'phone,case,cover,belt,wear' => Phones\n",
      "'top' => Clothing\n",
      "'phone,battery,motorola,samsung,compared' => Phones\n",
      "'belt,cover,small' => Phones\n",
      "'shirt' => Clothing\n",
      "'dockers,coffee' => Home\n",
      "'under armour,year' => Phones\n",
      "'shirt,dress,ordered,red' => Clothing\n",
      "'motorola,phone,nokia' => Phones\n",
      "'charger' => Phones\n",
      "'phone,case' => Phones\n",
      "'phone,bluetooth,battery' => Phones\n",
      "'phone,battery' => Phones\n",
      "'phone' => Phones\n",
      "'phone,~3,year,verizon,battery,charger' => Phones\n",
      "'case' => Phones\n",
      "'cookware,pan' => Home\n",
      "'display' => Phones\n",
      "'phone' => Phones\n",
      "'display,large,red,stored,warmer' => Home\n",
      "'year' => Phones\n",
      "'wear,shirt,dress,ordered' => Clothing\n",
      "'phone' => Phones\n",
      "'phone' => Phones\n",
      "'year' => Phones\n",
      "'ordered,charger,nokia,phone' => Phones\n",
      "'phone,black,sony,cable,charger' => Phones\n",
      "'charger' => Phones\n",
      "'coffee,warmer,year,young,mug,merry,black' => Home\n",
      "'coffee,mug' => Home\n",
      "'top' => Clothing\n",
      "'samsung,battery,year' => Phones\n",
      "'coffee,black,baby.i' => Home\n",
      "'pant,maroon,ordered,size,year,wear' => Phones\n",
      "'cooker,small,japan' => Home\n",
      "'shirt,ordered,sleeveless' => Clothing\n",
      "'shirt' => Clothing\n",
      "'shirt,sleeveless,tank-top,' => Clothing\n",
      "'motorola,phone,battery,year' => Phones\n",
      "'ordered' => Phones\n",
      "'display,red,verizon,phone,motorola' => Phones\n",
      "'ordered' => Phones\n",
      "'phone' => Phones\n",
      "'phone,year' => Phones\n",
      "'cover' => Phones\n",
      "'year' => Phones\n",
      "'year' => Phones\n",
      "'pant,fabric' => Clothing\n",
      "'cable,near' => Phones\n",
      "'phone,year,verizon' => Phones\n",
      "'case,phone,top' => Phones\n",
      "'wear,shirt,dress,ordered' => Clothing\n",
      "'coffee' => Home\n",
      "'ordered,year' => Phones\n",
      "'boxwave,bluetooth,china,case,battery,year' => Home\n",
      "'fan' => Home\n",
      "'phone,large' => Phones\n",
      "'phone,battery' => Phones\n",
      "'year' => Phones\n",
      "'headset,case,charger' => Phones\n",
      "'cooker,battery' => Home\n",
      "'phone,case' => Phones\n",
      "'size' => Phones\n",
      "'shirt,size,ordered' => Clothing\n",
      "'tee,size,large,shirt,ordered' => Clothing\n",
      "'shirt' => Clothing\n",
      "'coffee,black,mug' => Home\n",
      "'case,phone,cover,nokia' => Phones\n",
      "'phone,motorola,nokia' => Phones\n",
      "'blackberry' => Phones\n",
      "'china' => Home\n",
      "'phone,ear,battery' => Phones\n",
      "'headset,nokia,phone,ordered' => Phones\n",
      "'shirt,small,12-year' => Clothing\n",
      "'phone,year' => Phones\n",
      "'phone,motorola,nokia,sony' => Phones\n",
      "'blue,wear,fabric,cotton,shirt' => Clothing\n",
      "'dockers,coffee' => Home\n",
      "'phone,charger' => Phones\n",
      "'ordered,large,size,small' => Phones\n",
      "'motorola,phone' => Phones\n",
      "'charger' => Phones\n",
      "'shirt,large,xl' => Clothing\n",
      "'warmer,coffee' => Home\n",
      "'phone' => Phones\n",
      "'ordered,black,size,wear,large' => Phones\n",
      "'phone' => Phones\n",
      "'shirt,nylon,wear' => Clothing\n",
      "'phone,ear,blackberry,motorola' => Phones\n",
      "'phone' => Phones\n",
      "'year' => Phones\n",
      "'nylon' => Clothing\n",
      "'large' => Clothing\n",
      "'phone,motorola' => Phones\n",
      "'cover,phone' => Phones\n",
      "'samsung,phone' => Phones\n",
      "'nokia,phone,small' => Phones\n",
      "'cover,ear,earpiece,phone,case,size' => Phones\n",
      "'coffee,small' => Home\n",
      "'bluetooth' => Phones\n",
      "'pan' => Home\n",
      "'case' => Phones\n",
      "'small' => Phones\n",
      "'pan' => Home\n",
      "'phone,blackberry' => Phones\n",
      "'phone,battery' => Phones\n",
      "'cable,charger' => Phones\n",
      "'ear piece,year,nokia,phone,ear' => Phones\n",
      "'cookware' => Home\n",
      "'shirt,wear' => Clothing\n",
      "'sony,phone' => Phones\n",
      "'phone,ordered,top' => Phones\n",
      "'phone' => Phones\n",
      "'shirt,fabric' => Clothing\n",
      "'cable' => Phones\n",
      "'bluetooth' => Phones\n",
      "'phone,battery' => Phones\n",
      "'large,coffee,mug' => Home\n",
      "'figured,large' => Clothing\n",
      "'phone' => Phones\n",
      "'phone' => Phones\n",
      "'ordered' => Phones\n",
      "'coffee,top,case,year' => Phones\n",
      "'phone,ear,battery' => Phones\n",
      "'fabric' => Clothing\n",
      "'wear' => Phones\n",
      "'shoe,small,size' => Footwear\n",
      "'case,year' => Phones\n",
      "'nokia,ear,phone' => Phones\n",
      "'phone,year,blog,bluetooth,blue,headset' => Phones\n",
      "'top,size,fabric,loss,cover,curtain' => Clothing\n",
      "'dockers,coffee' => Home\n",
      "'headset' => Phones\n",
      "'phone,blackberry' => Phones\n",
      "'small,coffee,top' => Home\n",
      "'case,belt,verizon' => Phones\n",
      "'warmer' => Home\n",
      "'year,coffee' => Home\n",
      "'shirt' => Clothing\n",
      "'phone,display' => Phones\n",
      "'year,cord' => Phones\n",
      "'coffee' => Home\n",
      "'tee,size,large,shirt,ordered' => Clothing\n",
      "'shirt,size,ordered' => Clothing\n",
      "'coffee,year' => Home\n",
      "'phone,verizon' => Phones\n",
      "'phone,case,ordered,loss' => Phones\n",
      "'mug,case,warmer' => Home\n"
     ]
    }
   ],
   "source": [
    "#Routing the reviews to the corresponding Department using Naive Bayes\n",
    "#CountVectorizer is used to count the number of times a word occurs in a corpus\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(df_train['meaning'])\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, df_train['department'])\n",
    "predicted=[]\n",
    "docs_new=pd.read_csv(path+\"test.csv\")\n",
    "docs_new1=docs_new[\"Entity\"]\n",
    "docs_new2=docs_new[\"Label\"]\n",
    "docs_new3=docs_new[\"Original_review\"]\n",
    "docs_new=pd.concat([docs_new1,docs_new2,docs_new3],axis=1)\n",
    "docs_new = docs_new.dropna()\n",
    "for i in docs_new[\"Entity\"]:\n",
    "    #print(\"i:\",i)\n",
    "    l=[]\n",
    "    x=i.split(',')\n",
    "    for ele in x:\n",
    "        w=wn.synsets(ele)\n",
    "    \n",
    "        if(len(w)==0):\n",
    "            l=[i]    \n",
    "        else:\n",
    "            string=ele+\" \"+w[0].definition()\n",
    "            s=[w.lower() for w in string.split(' ') if w.lower() not in stop_words]\n",
    "            l.append(' '.join(s))\n",
    "    l=','.join(l)\n",
    "    X_new_counts = count_vect.transform([l])\n",
    "    X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "    predicted.append(clf.predict(X_new_tfidf))\n",
    "\n",
    "with open(path+\"named_entities.csv\", 'w') as writeFile:\n",
    "    writer = csv.writer(writeFile)\n",
    "    header=[\"Entities\",\"Label\",\"Department\",\"Original_review\"]\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    for doc, label,category,rev in zip(docs_new[\"Entity\"],docs_new[\"Label\"], predicted,docs_new[\"Original_review\"]):\n",
    "        named_entities=[]\n",
    "        print('%r => %s' % (doc,Departments[category[0]]))\n",
    "        named_entities=[doc,label,Departments[category[0]],rev]\n",
    "        writer = csv.writer(writeFile)\n",
    "        writer.writerow(named_entities)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ? Could you specify if it is men's wear or women's?\n",
      "We are sorry for the inconvenience. Please provide your contact details. Our customer support will get in touch with you shortly.?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ? Could you specify if it is men's wear or women's?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Please provide your contact details. Our customer support will get in touch with you shortly.?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ? Could you specify if it is men's wear or women's?\n",
      "We are sorry for the inconvenience. Please provide your contact details. Our customer support will get in touch with you shortly.?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Please provide your contact details. Our customer support will get in touch with you shortly.?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ? Could you specify if it is men's wear or women's?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Please provide your contact details. Our customer support will get in touch with you shortly.?\n",
      "We are sorry for the inconvenience. Please provide your contact details. Our customer support will get in touch with you shortly.?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Please provide your contact details. Our customer support will get in touch with you shortly.?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ? Could you specify if it is men's wear or women's?\n",
      "We are sorry for the inconvenience. Please provide your contact details. Our customer support will get in touch with you shortly.?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ? Could you specify if it is men's wear or women's?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ? Could you specify if it is men's wear or women's?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ? Could you specify if it is men's wear or women's?\n",
      "We are sorry for the inconvenience. Please provide your contact details. Our customer support will get in touch with you shortly.?\n",
      "We are sorry for the inconvenience. the product name ,the brand ?\n",
      "We are sorry for the inconvenience. Please provide your contact details. Our customer support will get in touch with you shortly.?\n",
      "We are sorry for the inconvenience. the product name ,the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ? Could you specify if it is men's wear or women's?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Please provide your contact details. Our customer support will get in touch with you shortly.?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ? Could you specify if it is men's wear or women's?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Please provide your contact details. Our customer support will get in touch with you shortly.?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. the product name ,the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ? Could you specify if it is men's wear or women's?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ? Could you specify if it is men's wear or women's?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ? Could you specify if it is men's wear or women's?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Please provide your contact details. Our customer support will get in touch with you shortly.?\n",
      "We are sorry for the inconvenience. Please provide your contact details. Our customer support will get in touch with you shortly.?\n",
      "We are sorry for the inconvenience. Could you please provide the product name ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Please provide your contact details. Our customer support will get in touch with you shortly.?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ? Could you specify if it is men's wear or women's?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Please provide your contact details. Our customer support will get in touch with you shortly.?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ? Could you specify if it is men's wear or women's?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. the product name ,the brand ?\n",
      "We are sorry for the inconvenience. Please provide your contact details. Our customer support will get in touch with you shortly.?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ? Could you specify if it is men's wear or women's?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ? Could you specify if it is men's wear or women's?\n",
      "We are sorry for the inconvenience. Please provide your contact details. Our customer support will get in touch with you shortly.?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. the product name ,the brand ? Could you specify if it is men's wear or women's?\n",
      "We are sorry for the inconvenience. the product name ,the brand ? Could you specify if it is men's wear or women's?\n",
      "We are sorry for the inconvenience. Please provide your contact details. Our customer support will get in touch with you shortly.?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Please provide your contact details. Our customer support will get in touch with you shortly.?\n",
      "We are sorry for the inconvenience. Please provide your contact details. Our customer support will get in touch with you shortly.?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. the product name ,the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Please provide your contact details. Our customer support will get in touch with you shortly.?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Please provide your contact details. Our customer support will get in touch with you shortly.?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ? Could you specify if it is men's wear or women's?\n",
      "We are sorry for the inconvenience. Please provide your contact details. Our customer support will get in touch with you shortly.?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ? Could you specify if it is men's wear or women's?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. the product name ,the brand ? Could you specify if it is men's wear or women's?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. the product name ,the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ? Could you specify if it is men's wear or women's?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Please provide your contact details. Our customer support will get in touch with you shortly.?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ? Could you specify if it is men's wear or women's?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Please provide your contact details. Our customer support will get in touch with you shortly.?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Please provide your contact details. Our customer support will get in touch with you shortly.?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ? Could you specify if it is men's wear or women's?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ? Could you specify if it is men's wear or women's?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ? Could you specify if it is men's wear or women's?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Please provide your contact details. Our customer support will get in touch with you shortly.?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n",
      "We are sorry for the inconvenience. Could you please provide the brand ?\n"
     ]
    }
   ],
   "source": [
    "#Generating an automated response based on the entities recognised\n",
    "df=pd.read_csv(path+\"named_entities.csv\", delimiter = ',')\n",
    "#print(x['Entities'])\n",
    "#print(x['label'])\n",
    "\n",
    "with open(path+\"response.csv\",'w') as writeFile:\n",
    "    header=[\"Response\"]\n",
    "    writer = csv.writer(writeFile)\n",
    "    writer.writerow(header)\n",
    "    for x in range(len(df)):\n",
    "        #print(df.iloc[x])\n",
    "        brand=0\n",
    "        product=0\n",
    "        info=0\n",
    "        cloth=0\n",
    "        size=0\n",
    "        gender=0\n",
    "        b=[]\n",
    "        p=[]\n",
    "        inf=[]\n",
    "        #print(x,df.iloc[x]['Label'])\n",
    "        labels=df.iloc[x]['Label'].split(',')\n",
    "        #print(labels)\n",
    "        entities=df.iloc[x]['Entities'].split(',')\n",
    "        for j in range(len(labels)):\n",
    "            #print(j,labels[j])\n",
    "            if labels[j]==\"BRAND\":\n",
    "                brand=1\n",
    "                b.append(entities[j])\n",
    "            elif labels[j]==\"PRODUCT\":\n",
    "                product=1\n",
    "                p.append(entities[j])\n",
    "            #if df.iloc[x]['Department']==\"Phones\":\n",
    "            elif labels[j]==\"INFO\":\n",
    "                info=1\n",
    "                inf.append(entities[j])\n",
    "            if df.iloc[x]['Department']==\"Clothing\":\n",
    "                cloth=1\n",
    "                if labels[j]==\"SIZE\":\n",
    "                    size=1\n",
    "                if labels[j]==\"GENDER\":\n",
    "                    gender=1\n",
    "            #    if labels[j]==\"I\"\n",
    "            #elif df.iloc[x]['Department']==\"Home\":\n",
    "            #else if df.iloc[x]['Department']==\"Footwear\":'''\n",
    "        s=\"We are sorry for the inconvenience. \"\n",
    "        pro=\"Could you please provide \"\n",
    "        sp=\"\"\n",
    "        sb=\"\"\n",
    "        sg=\"\"\n",
    "        if brand==1 and product==1:\n",
    "            pro=\"\"\n",
    "            s=\"We are sorry for the inconvenience. Please provide your contact details. Our customer support will get in touch with you shortly.\"\n",
    "        if product==0:\n",
    "            sp=\"the product name \"\n",
    "        if brand==0:\n",
    "            if(sp!=\"\"):\n",
    "                sb=\",the brand \"\n",
    "            else:\n",
    "                sb=\"the brand \"\n",
    "        if gender==0 and cloth==1:\n",
    "            sg=\" Could you specify if it is men's wear or women's?\"\n",
    "        if(product==brand==0):\n",
    "            pro=\"\"\n",
    "        res=s+pro+sp+sb+\"?\"+sg\n",
    "        print(res)\n",
    "        writer.writerow([res])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formulating the final results-review,entities recognised, Department to which the query was routed to, Automated response\n",
    "df=pd.read_csv(path+\"named_entities.csv\", delimiter = ',')\n",
    "df1=pd.read_csv(path+\"response.csv\", delimiter=',')\n",
    "docs_new1=df[\"Original_review\"]\n",
    "docs_new2=df[\"Entities\"]\n",
    "docs_new3=df[\"Department\"]\n",
    "docs_new4=df[\"Label\"]\n",
    "docs_new5=df1[\"Response\"]\n",
    "\n",
    "docs_new=pd.concat([docs_new1,docs_new2,docs_new3,docs_new4,docs_new5],axis=1)\n",
    "docs_new = docs_new.dropna()\n",
    "with open(path+\"result.csv\", 'w') as writeFile:\n",
    "    writer = csv.writer(writeFile)\n",
    "    header=[\"Review\",\"Entities\",\"Label\",\"Department\", \"Response\"]\n",
    "    writer.writerow(header)\n",
    "    for review, entity,department,label,response in zip(docs_new[\"Original_review\"],docs_new[\"Entities\"],docs_new[\"Label\"], docs_new[\"Department\"],docs_new[\"Response\"]):\n",
    "        named_entities=[]\n",
    "        #print('%r => %s' % (doc,Departments[category[0]]))\n",
    "        named_entities=[review,entity,department,label,response]\n",
    "        writer = csv.writer(writeFile)\n",
    "        writer.writerow(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(path+\"result.csv\")\n",
    "#Retrieve 4th row for clothing\n",
    "l=[df.iloc[4].values,df.iloc[13].values,df.iloc[34].values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printmd(string,color=None):\n",
    "    colorstr = \"<span style='color:{}'>{}</span>\".format(color, string)\n",
    "    display(Markdown(colorstr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<span style='color:teal'>**Review:**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i wore this shirt just once. it's almost transparent. not worth it even if you get it for free.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:teal'>**Entities:**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shirt\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:teal'>**NER tag:**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRODUCT\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:teal'>**Department:**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clothing"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:teal'>**Response:**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are sorry for the inconvenience. Could you please provide the brand ? Could you specify if it is men's wear or women's?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:teal'>**Review:**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in fact i bought it in september for friend as a gift , so the phone deliver to friends address directly , i did not see it till 2 -3 month later ,it was such a shame to give such a bad gift to friend !it can not work at all ,and its looking like the refresh old phone , so i try to connect the seller for return , but they told that we already expire the warranty , we only could send back the phone for repair ! but who want the refresh old phone ? i paid 288 usd for the new phone price , but they sent me trash ..........\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:teal'>**Entities:**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phone\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:teal'>**NER tag:**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRODUCT\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:teal'>**Department:**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phones"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:teal'>**Response:**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are sorry for the inconvenience. Could you please provide the brand ?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:teal'>**Review:**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love the features of this rice cooker. it cooks and re-heat brown rice very well.i purchased the rice cooker last summer and have been used it every day. yesterday, i noticed that two small spots on inner non-stick layer was chipped. the non-stick outer layer started to chip earlier. i can not believed it happened to this expensive rice cooker that is made in japan.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:teal'>**Entities:**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cooker,small,japan\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:teal'>**NER tag:**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRODUCT,INFO,PRODUCT\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:teal'>**Department:**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:teal'>**Response:**</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are sorry for the inconvenience. Could you please provide the brand ?\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(l)):\n",
    "    printmd(\"**Review:**\",color=\"teal\")\n",
    "    print(l[i][0],end=\"\")\n",
    "    printmd(\"**Entities:**\",color=\"teal\")\n",
    "    print(l[i][1])\n",
    "    printmd(\"**NER tag:**\",color=\"teal\")\n",
    "    print(l[i][2])\n",
    "    printmd(\"**Department:**\",color=\"teal\")\n",
    "    print(l[i][3],end=\"\")\n",
    "    printmd(\"**Response:**\",color=\"teal\")\n",
    "    print(l[i][4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
